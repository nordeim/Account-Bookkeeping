# File: app/services/audit_services.py
from abc import ABC, abstractmethod
from typing import List, Optional, Any, TYPE_CHECKING, Dict, Tuple, Union # Ensure Union is imported
from sqlalchemy import select, func, and_, or_
from sqlalchemy.orm import selectinload, aliased
from datetime import datetime, date # date might not be used directly but good to have with datetime

from app.core.database_manager import DatabaseManager
from app.models.audit.audit_log import AuditLog
from app.models.audit.data_change_history import DataChangeHistory
from app.models.core.user import User 
from app.services import IRepository # Base repository interface
from app.utils.pydantic_models import AuditLogEntryData, DataChangeHistoryEntryData
# from app.utils.result import Result # Not typically returned by services, but managers
from app.common.enums import DataChangeTypeEnum

if TYPE_CHECKING:
    from app.core.application_core import ApplicationCore
    # from sqlalchemy.ext.asyncio import AsyncSession # If session is passed around

# Define Interfaces (can also be in __init__.py, but here for clarity with service)
class IAuditLogRepository(IRepository[AuditLog, int]):
    @abstractmethod
    async def get_audit_logs_paginated(
        self,
        user_id_filter: Optional[int] = None,
        action_filter: Optional[str] = None,
        entity_type_filter: Optional[str] = None,
        entity_id_filter: Optional[int] = None,
        start_date_filter: Optional[datetime] = None,
        end_date_filter: Optional[datetime] = None,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[AuditLogEntryData], int]: # Returns list of DTOs and total count
        pass

class IDataChangeHistoryRepository(IRepository[DataChangeHistory, int]):
    @abstractmethod
    async def get_data_change_history_paginated(
        self,
        table_name_filter: Optional[str] = None,
        record_id_filter: Optional[int] = None,
        changed_by_user_id_filter: Optional[int] = None,
        start_date_filter: Optional[datetime] = None,
        end_date_filter: Optional[datetime] = None,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[DataChangeHistoryEntryData], int]: # Returns list of DTOs and total count
        pass


class AuditLogService(IAuditLogRepository, IDataChangeHistoryRepository): 
    def __init__(self, db_manager: "DatabaseManager", app_core: Optional["ApplicationCore"] = None):
        self.db_manager = db_manager
        self.app_core = app_core # app_core can be used for accessing logger or other global components

    # --- Standard IRepository Stubs (Audit logs are mostly read-only from app perspective) ---
    async def get_by_id(self, id_val: int) -> Optional[Union[AuditLog, DataChangeHistory]]:
        # This method might be less useful for audit logs which are usually queried by context, not ID.
        # However, implementing for interface compliance or specific edge cases.
        async with self.db_manager.session() as session:
            log_entry = await session.get(AuditLog, id_val)
            if log_entry:
                # Could convert to DTO here if consistency is desired, but service usually returns ORM
                return log_entry
            history_entry = await session.get(DataChangeHistory, id_val)
            return history_entry # Could also be DTO
            
    async def get_all(self) -> List[Union[AuditLog, DataChangeHistory]]:
        # Not practical for potentially huge audit tables. Use paginated methods.
        raise NotImplementedError("Use paginated methods for audit logs. Get_all is not supported.")

    async def add(self, entity: Union[AuditLog, DataChangeHistory]) -> Union[AuditLog, DataChangeHistory]:
        # Audit logs are typically generated by database triggers or specialized logging calls, not generic add.
        raise NotImplementedError("Audit logs are system-generated and cannot be added via this service.")

    async def update(self, entity: Union[AuditLog, DataChangeHistory]) -> Union[AuditLog, DataChangeHistory]:
        # Audit logs should be immutable.
        raise NotImplementedError("Audit logs are immutable and cannot be updated.")

    async def delete(self, id_val: int) -> bool:
        # Audit logs should generally not be deleted programmatically, archiving is preferred.
        raise NotImplementedError("Audit logs should not be deleted programmatically via this service.")

    # --- Custom Service Methods for Audit Logs ---

    def _format_changes_summary(self, changes_jsonb: Optional[Dict[str, Any]]) -> Optional[str]:
        if not changes_jsonb:
            return None
        
        summary_parts = []
        old_data = changes_jsonb.get('old')
        new_data = changes_jsonb.get('new')

        action = "Modified" # Default action
        if isinstance(new_data, dict) and old_data is None:
            action = "Created"
        elif new_data is None and isinstance(old_data, dict):
            action = "Deleted"
        
        if action == "Created" and isinstance(new_data, dict):
            # For created records, list some key fields if possible, or just confirm creation
            # Example: List up to 3 non-sensitive fields.
            keys_to_show = [k for k in new_data.keys() if k not in ["id", "created_at", "updated_at", "password_hash", "logo"]][:3]
            if keys_to_show:
                details = ", ".join([f"{k}: '{str(new_data[k])[:30]}...'" if len(str(new_data[k])) > 30 else f"{k}: '{str(new_data[k])}'" for k in keys_to_show])
                summary_parts.append(f"Record {action}. Details: {details}")
            else:
                summary_parts.append(f"Record {action}.")
        elif action == "Deleted" and isinstance(old_data, dict):
            summary_parts.append(f"Record {action}.")
        elif action == "Modified" and isinstance(new_data, dict) and isinstance(old_data, dict):
            changed_fields_count = 0
            for key, new_val in new_data.items():
                if key in ["created_at", "updated_at", "password_hash", "logo"]: # Skip noisy or sensitive fields
                    continue 
                old_val = old_data.get(key) # Use .get() in case old_data doesn't have the key (new field added to schema)
                
                # Handle cases where a value might be a list or dict itself (e.g. JSONB field)
                # For simplicity, we'll compare their string representations for this summary
                str_old_val = str(old_val)
                str_new_val = str(new_val)

                if str_old_val != str_new_val: 
                    changed_fields_count += 1
                    if changed_fields_count <= 3: # Show details for first few changes
                        summary_parts.append(
                            f"'{key}': '{str_old_val[:30]}...' -> '{str_new_val[:30]}...'" 
                            if len(str_old_val) > 30 or len(str_new_val) > 30 
                            else f"'{key}': '{str_old_val}' -> '{str_new_val}'"
                        )
            if changed_fields_count > 3:
                summary_parts.append(f"...and {changed_fields_count - 3} other field(s).")
            if not summary_parts and changed_fields_count == 0 : # Only timestamps / sensitive fields changed
                 summary_parts.append("Minor updates (e.g., timestamps).")

        return "; ".join(summary_parts) if summary_parts else "No changes detailed or only sensitive fields updated."


    async def get_audit_logs_paginated(
        self,
        user_id_filter: Optional[int] = None,
        action_filter: Optional[str] = None,
        entity_type_filter: Optional[str] = None,
        entity_id_filter: Optional[int] = None,
        start_date_filter: Optional[datetime] = None,
        end_date_filter: Optional[datetime] = None,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[AuditLogEntryData], int]:
        async with self.db_manager.session() as session:
            conditions = []
            if user_id_filter is not None:
                conditions.append(AuditLog.user_id == user_id_filter)
            if action_filter:
                conditions.append(AuditLog.action.ilike(f"%{action_filter}%")) # Case-insensitive search
            if entity_type_filter:
                conditions.append(AuditLog.entity_type.ilike(f"%{entity_type_filter}%"))
            if entity_id_filter is not None:
                conditions.append(AuditLog.entity_id == entity_id_filter)
            if start_date_filter:
                conditions.append(AuditLog.timestamp >= start_date_filter)
            if end_date_filter: # Add 1 day to end_date_filter to make it inclusive of the whole day
                conditions.append(AuditLog.timestamp < (end_date_filter + datetime.timedelta(days=1)))


            UserAlias = aliased(User, name="user_alias") # Explicit alias name
            
            # Count total matching records
            count_stmt = select(func.count(AuditLog.id))
            if conditions:
                count_stmt = count_stmt.where(and_(*conditions))
            total_count_res = await session.execute(count_stmt)
            total_count = total_count_res.scalar_one()

            # Fetch paginated records
            stmt = select(AuditLog, UserAlias.username.label("username")).outerjoin(
                UserAlias, AuditLog.user_id == UserAlias.id # Ensure correct join condition
            )
            if conditions:
                stmt = stmt.where(and_(*conditions))
            
            stmt = stmt.order_by(AuditLog.timestamp.desc()) # type: ignore[attr-defined]
            if page_size > 0: # Allow page_size=0 or negative to fetch all (though not recommended for large tables)
                 stmt = stmt.limit(page_size).offset((page - 1) * page_size)
            
            result = await session.execute(stmt)
            log_entries_data: List[AuditLogEntryData] = []
            
            for row_mapping in result.mappings().all(): # Use mappings() for dict-like access
                log_orm = row_mapping[AuditLog]
                username = row_mapping["username"] if row_mapping["username"] else \
                           (f"User ID: {log_orm.user_id}" if log_orm.user_id else "System/Unknown")
                
                log_entries_data.append(AuditLogEntryData(
                    id=log_orm.id,
                    timestamp=log_orm.timestamp,
                    username=username,
                    action=log_orm.action,
                    entity_type=log_orm.entity_type,
                    entity_id=log_orm.entity_id,
                    entity_name=log_orm.entity_name,
                    changes_summary=self._format_changes_summary(log_orm.changes),
                    ip_address=log_orm.ip_address
                ))
            return log_entries_data, total_count

    async def get_data_change_history_paginated(
        self,
        table_name_filter: Optional[str] = None,
        record_id_filter: Optional[int] = None,
        changed_by_user_id_filter: Optional[int] = None,
        start_date_filter: Optional[datetime] = None,
        end_date_filter: Optional[datetime] = None,
        page: int = 1,
        page_size: int = 50
    ) -> Tuple[List[DataChangeHistoryEntryData], int]:
        async with self.db_manager.session() as session:
            conditions = []
            if table_name_filter:
                conditions.append(DataChangeHistory.table_name.ilike(f"%{table_name_filter}%"))
            if record_id_filter is not None:
                conditions.append(DataChangeHistory.record_id == record_id_filter)
            if changed_by_user_id_filter is not None:
                conditions.append(DataChangeHistory.changed_by == changed_by_user_id_filter)
            if start_date_filter:
                conditions.append(DataChangeHistory.changed_at >= start_date_filter)
            if end_date_filter: # Add 1 day to end_date_filter to make it inclusive of the whole day
                conditions.append(DataChangeHistory.changed_at < (end_date_filter + datetime.timedelta(days=1)))


            UserAlias = aliased(User, name="changed_by_user_alias") # Explicit alias name

            # Count total matching records
            count_stmt = select(func.count(DataChangeHistory.id))
            if conditions:
                count_stmt = count_stmt.where(and_(*conditions))
            total_count_res = await session.execute(count_stmt)
            total_count = total_count_res.scalar_one()

            # Fetch paginated records
            stmt = select(DataChangeHistory, UserAlias.username.label("changed_by_username")).outerjoin(
                UserAlias, DataChangeHistory.changed_by == UserAlias.id # Ensure correct join condition
            )
            if conditions:
                stmt = stmt.where(and_(*conditions))
            
            stmt = stmt.order_by(DataChangeHistory.changed_at.desc()) # type: ignore[attr-defined]
            if page_size > 0:
                 stmt = stmt.limit(page_size).offset((page - 1) * page_size)

            result = await session.execute(stmt)
            history_entries_data: List[DataChangeHistoryEntryData] = []

            for row_mapping in result.mappings().all(): # Use mappings() for dict-like access
                hist_orm = row_mapping[DataChangeHistory]
                username = row_mapping["changed_by_username"] if row_mapping["changed_by_username"] else \
                           (f"User ID: {hist_orm.changed_by}" if hist_orm.changed_by else "System/Unknown")
                
                history_entries_data.append(DataChangeHistoryEntryData(
                    id=hist_orm.id,
                    changed_at=hist_orm.changed_at,
                    table_name=hist_orm.table_name,
                    record_id=hist_orm.record_id,
                    field_name=hist_orm.field_name,
                    old_value=hist_orm.old_value,
                    new_value=hist_orm.new_value,
                    change_type=DataChangeTypeEnum(hist_orm.change_type), # Convert string from DB to Enum
                    changed_by_username=username
                ))
            return history_entries_data, total_count
